verbose: True # Whether to print info.
log_full_detail: False # if True, logs a more detailed information to tensorboard.

# DiffDrive Environment Specific Parameters
distance_threshold: 0.1 # if robot is withing the reach of target smaller than this threshold value then goal state is reached
render_mode: ['no_render'] # specifies which sensory informations to visualize. Currently supports ["no_render", "draw_coordinates"]. Corresponds to entries specified in ForkliftEnv.metadata['render_modes'] for more information.
# render_mode: ['draw_coordinates'] # specifies which sensory informations to visualize. Currently supports ["no_render", "draw_coordinates"]. Corresponds to entries specified in ForkliftEnv.metadata['render_modes'] for more information.
step_duration: 50_000_000 # specifies the duration of each step() with respect to simulation time in nanoseconds (e.g. 1 second is 1_000_000_000).
real_time: False # if True than simulation is run in real time

use_diff_drive_kinematics: False # If True env uses diff drive kinematics, else simplified version where agent can move freely will be used.
l: 0.5  # distance between the centers of the two wheels. Only used when use_diff_drive_kinematics = True.

# ========== HYPERPARAMTERS sb3 PPO: ============= 
mode: "train" # ['train', 'test', 'hyperparameter_search'] # TODO: set from config
# mode: "test" # ['train', 'test', 'hyperparameter_search'] # TODO: set from config
total_timesteps: 5_000_000 # total number of episodes to train
max_episode_length: 500 # maximum iteration length of an episode.
# warmup_steps: 100 # number of steps to take random actions before using the agents policy
update_every: 2048 # update model after every update_every steps taken.
num_updates: 10 # at every update_every steps while updating the model perform num_updates many updates by sampling a new batch for each of them. 
actor_hidden_dims: [400, 300] # holds dimensions of the hidden layers excluding the input layer and the input and the output dimensions for the actor network of ddpg agent.
critic_hidden_dims: [400, 300] # holds dimensions of the hidden layers excluding the input layer and the input and the output dimensions for the critic network of ddpg agent.
lr: 0.0003 # learning rate
# critic_lr: 1e-3 # learning rate for the Critic Network
# initial_epsilon: 1.0 # epsilon used for multiplying the gaussian distribution sample to obtain the noise to add to the agent's action (exploration).
# epsilon_decay: 0.9999 # every time an action is taken epsilon is reduced by this amount (i.e. epsilon *= epsilon_decay).
# min_epsilon: 0.05 # minimum value that epsilon can decay to
# act_noise: 0.3 # stddev for Gaussian exploration noise auded to policy at training time. (At test time, no noise is added.)
# target_noise: 0.2 # stddev for smoothing noise added to target policy.
# clip_noise_range: 0.5 # noise is clipped btw [-clip_noise_range, clip_noise_range] before being added to the action
gamma: 0.9999 # next state discount rate
# tau: 0.995 # tau value used in updating the target networks using polyak averaging (i.e. targ_net = tau*targ_net + (1-tau) * net).
# policy_update_delay: 1 # Actor is updated after every policy_update_delay number of updates
batch_size: 64
seed: 42 # used to set seed for reproducibility
# save_every: 100 # specifies after every how many updates to the model the agent model is saved. Model is saved at the end of 'total_episodes' episode.
# ======================================= 